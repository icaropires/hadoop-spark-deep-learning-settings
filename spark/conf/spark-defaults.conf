#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
spark.master                       yarn
spark.submit.deployMode            cluster
spark.jars.packages                databricks:spark-deep-learning:1.1.0-spark2.3-s_2.11

spark.driver.host                  master
spark.driver.memory                2g
spark.driver.cores                 3

spark.memory.fraction              0.9
spark.executor.cores               4
spark.executor.memory              12g
spark.executor.extraJavaOptions    -XX:+UseCompressedOops -XX:+UseG1GC -XX:G1HeapRegionSize=32

spark.yarn.am.memoryOverhead       2g
spark.executor.memoryOverhead      2g

spark.maxRemoteBlockSizeFetchToMem 1g

spark.shuffle.service.enabled      true
spark.serializer                   org.apache.spark.serializer.KryoSerializer

spark.dynamicAllocation.enabled              true
spark.dynamicAllocation.minExecutors         1
spark.dynamicAllocation.initialExecutors     1
spark.dynamicAllocation.maxExecutors         4

spark.eventLog.enabled             true
spark.logConf                      true
spark.eventLog.dir                 hdfs://master:9000/user/hadoop/logs
spark.history.fs.logDirectory      hdfs://master:9000/user/hadoop/logs

spark.yarn.appMasterEnv.PYSPARK_PYTHON <put_here_path_to_virtualenv_folder>/cluster/bin/python3

# UseCompressedOops: Use a policy that limits the proportion of the VM's time that is spent in GC before an OutOfMemory error is thrown. (Introduced in 6.)
# UseG1GC: It can improve performance in some situations where garbage collection is a bottleneck.
